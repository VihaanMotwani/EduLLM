{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP2nMf2MhENw3laEN5MaLMT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Reranking Methods in RAG Systems\n","\n","## Overview\n","Reranking is a crucial step in Retrieval-Augmented Generation (RAG) systems that aims to improve the relevance and quality of retrieved documents. It involves reassessing and reordering initially retrieved documents to ensure that the most pertinent information is prioritized for subsequent processing or presentation.\n","\n","## Motivation\n","The primary motivation for reranking in RAG systems is to overcome limitations of initial retrieval methods, which often rely on simpler similarity metrics. Reranking allows for more sophisticated relevance assessment, taking into account nuanced relationships between queries and documents that might be missed by traditional retrieval techniques. This process aims to enhance the overall performance of RAG systems by ensuring that the most relevant information is used in the generation phase.\n","\n","## Key Components\n","Reranking systems typically include the following components:\n","\n","1. Initial Retriever: Often a vector store using embedding-based similarity search.\n","2. Reranking Model: A Large Language Model (LLM) for scoring relevance\n","3. Scoring Mechanism: A method to assign relevance scores to documents\n","4. Sorting and Selection Logic: To reorder documents based on new scores\n","\n","## Method Details\n","The reranking process generally follows these steps:\n","\n","1. Initial Retrieval: Fetch an initial set of potentially relevant documents.\n","2. Pair Creation: Form query-document pairs for each retrieved document.\n","3. Scoring: Use prompts to ask the LLM to rate document relevance.\n","4. Score Interpretation: Parse and normalize the relevance scores.\n","5. Reordering: Sort documents based on their new relevance scores.\n","6. Selection: Choose the top K documents from the reordered list.\n","\n","## Benefits of this Approach\n","Reranking offers several advantages:\n","\n","1. Improved Relevance: By using more sophisticated models, reranking can capture subtle relevance factors.\n","2. Flexibility: Different reranking methods can be applied based on specific needs and resources.\n","3. Enhanced Context Quality: Providing more relevant documents to the RAG system improves the quality of generated responses.\n","4. Reduced Noise: Reranking helps filter out less relevant information, focusing on the most pertinent content.\n","\n","## Conclusion\n","Reranking is a powerful technique in RAG systems that significantly enhances the quality of retrieved information. Reranking allows for more nuanced and accurate assessment of document relevance. This improved relevance translates directly to better performance in downstream tasks, making reranking an essential component in advanced RAG implementations.\n"],"metadata":{"id":"5lYLhUXwzmMg"}},{"cell_type":"markdown","source":["# Package Installation and Imports"],"metadata":{"id":"PQoizENCzyX3"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"2VL4_tdSy9UV","executionInfo":{"status":"ok","timestamp":1752474408400,"user_tz":-480,"elapsed":19846,"user":{"displayName":"Ares","userId":"14709530049333005179"}},"outputId":"f0de0c68-c3b5-4536-e334-0b3de700631d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchain-community\n","  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n","Collecting langchain_openai\n","  Downloading langchain_openai-0.3.27-py3-none-any.whl.metadata (2.3 kB)\n","Collecting pypdf\n","  Downloading pypdf-5.8.0-py3-none-any.whl.metadata (7.1 kB)\n","Collecting faiss-cpu\n","  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.68)\n","Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.26)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n","Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n","  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n","Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n","  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n","Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.4)\n","Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n","  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n","Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n","Requirement already satisfied: openai<2.0.0,>=1.86.0 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.93.3)\n","Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.9.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n","  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n","Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.8)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.11.7)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (4.14.1)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.10.18)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (4.9.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (1.9.0)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (0.10.0)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (4.67.1)\n","Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n","  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.7.9)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.33.2)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n","  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n","Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_openai-0.3.27-py3-none-any.whl (70 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pypdf-5.8.0-py3-none-any.whl (309 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.7/309.7 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n","Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n","Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n","Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n","Installing collected packages: python-dotenv, pypdf, mypy-extensions, marshmallow, httpx-sse, faiss-cpu, typing-inspect, pydantic-settings, dataclasses-json, langchain_openai, langchain-community\n","Successfully installed dataclasses-json-0.6.7 faiss-cpu-1.11.0 httpx-sse-0.4.1 langchain-community-0.3.27 langchain_openai-0.3.27 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.10.1 pypdf-5.8.0 python-dotenv-1.1.1 typing-inspect-0.9.0\n"]}],"source":["!pip install langchain-community langchain_openai pypdf faiss-cpu"]},{"cell_type":"code","source":["import os\n","import sys\n","from dotenv import load_dotenv\n","from langchain.docstore.document import Document\n","from langchain.document_loaders import PyPDFLoader\n","from typing import List, Dict, Any, Tuple\n","from langchain_openai import ChatOpenAI,OpenAIEmbeddings\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.chains import RetrievalQA\n","from langchain_core.retrievers import BaseRetriever\n","from langchain.vectorstores import FAISS\n","from pydantic import BaseModel, Field\n","from langchain_core.prompts import ChatPromptTemplate\n","\n","# Load environment variables from a .env file\n","load_dotenv()\n","\n","# Set the OpenAI API key environment variable\n","os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"],"metadata":{"id":"ZwH0HvUP0XV8","executionInfo":{"status":"ok","timestamp":1752476144815,"user_tz":-480,"elapsed":1302,"user":{"displayName":"Ares","userId":"14709530049333005179"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["### Define the document's path"],"metadata":{"id":"lt0ggIa60_mq"}},{"cell_type":"code","source":["# Download required data files\n","import os\n","os.makedirs('data', exist_ok=True)\n","\n","# Download the PDF document used in this notebook\n","!wget -O data/Understanding_Climate_Change.pdf https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n","path = \"data/Understanding_Climate_Change.pdf\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-NbOwbOe1ALI","executionInfo":{"status":"ok","timestamp":1752474454539,"user_tz":-480,"elapsed":211,"user":{"displayName":"Ares","userId":"14709530049333005179"}},"outputId":"28213ee6-8735-4edf-8da9-872dd6ef56a8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-07-14 06:27:34--  https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 206372 (202K) [application/octet-stream]\n","Saving to: ‘data/Understanding_Climate_Change.pdf’\n","\n","\r          data/Unde   0%[                    ]       0  --.-KB/s               \rdata/Understanding_ 100%[===================>] 201.54K  --.-KB/s    in 0.02s   \n","\n","2025-07-14 06:27:34 (8.97 MB/s) - ‘data/Understanding_Climate_Change.pdf’ saved [206372/206372]\n","\n"]}]},{"cell_type":"markdown","source":["### Create a vector store"],"metadata":{"id":"9YQdDLAI1eu3"}},{"cell_type":"code","source":["def replace_t_with_space(list_of_documents):\n","    \"\"\"\n","    Replaces all tab characters ('\\t') with spaces in the page content of each document\n","\n","    Args:\n","        list_of_documents: A list of document objects, each with a 'page_content' attribute.\n","\n","    Returns:\n","        The modified list of documents with tab characters replaced by spaces.\n","    \"\"\"\n","\n","    for doc in list_of_documents:\n","        doc.page_content = doc.page_content.replace('\\t', ' ')  # Replace tabs with spaces\n","    return list_of_documents\n","\n","def encode_pdf(path, chunk_size=1000, chunk_overlap=200):\n","    # Load PDF documents\n","    loader = PyPDFLoader(path)\n","    documents = loader.load()\n","\n","    # Split documents into chunks\n","    text_splitter = RecursiveCharacterTextSplitter(\n","        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n","    )\n","    texts = text_splitter.split_documents(documents)\n","    cleaned_texts = replace_t_with_space(texts)\n","\n","    # Create embeddings and vector store\n","    embeddings = OpenAIEmbeddings()\n","    vectorstore = FAISS.from_documents(cleaned_texts, embeddings)\n","\n","    return vectorstore\n","\n","vectorstore = encode_pdf(path)"],"metadata":{"id":"MucXT3zN1eLz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Method 1: LLM based function to rerank the retrieved documents"],"metadata":{"id":"qRxCSqNL33hW"}},{"cell_type":"code","source":["class Rating(BaseModel):\n","    relevance_score: float=Field(...,description=\"The relevance score of a document to a query.\")\n","\n","def rerank_documents(query: str, docs: List[Document], top_n: int = 3) -> List[Document]:\n","    llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0,max_tokens=4000)\n","    structured_llm_grader = llm.with_structured_output(Rating)\n","\n","    system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n","    On a scale of 1-10, rate the relevance of the following document to the query. Consider the specific context and intent of the query, not just keyword matches.\"\"\"\n","    grade_prompt = ChatPromptTemplate.from_messages(\n","        [\n","            (\"system\", system),\n","            (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n","        ]\n","    )\n","\n","    llm_chain = grade_prompt | structured_llm_grader\n","\n","    scored_doc = []\n","    for doc in docs:\n","      input = {\"document\": doc.page_content, \"question\": query}\n","      score = llm_chain.invoke(input).relevance_score\n","      try:\n","        score = float(score)\n","      except ValueError:\n","        score = 0  # Default score if parsing fails\n","      scored_doc.append((doc,score))\n","    ranked_score = sorted(scored_doc, key=lambda x:x[1], reverse=True)\n","    return [doc for doc, _ in ranked_score[:top_n]]"],"metadata":{"id":"QhPuvaCR36aR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Create a custom retriever based on our reranker"],"metadata":{"id":"IAwPeFomj4BQ"}},{"cell_type":"code","source":["# Create a custom retriever class\n","class CustomRetriever(BaseRetriever, BaseModel):\n","    vectorstore: Any = Field(description=\"Vector store for initial retrieval\")\n","\n","    class Config:\n","        arbitrary_types_allowed = True\n","\n","    def get_relevant_documents(self, query: str, num_docs=3) -> List[Document]:\n","        initial_docs = self.vectorstore.similarity_search(query, k=30)\n","        return rerank_documents(query, initial_docs, top_n=num_docs)\n","\n","# Create the custom retriever\n","custom_retriever = CustomRetriever(vectorstore=vectorstore)\n","# Create an LLM for answering questions\n","llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n","\n","# Create the RetrievalQA chain with the custom retriever\n","qa_chain = RetrievalQA.from_chain_type(\n","    llm=llm,\n","    chain_type=\"stuff\",\n","    retriever=custom_retriever,\n","    return_source_documents=True\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SyPm26Wjj4l8","executionInfo":{"status":"ok","timestamp":1752474849210,"user_tz":-480,"elapsed":13,"user":{"displayName":"Ares","userId":"14709530049333005179"}},"outputId":"9d91a84c-b3a4-468c-f70e-a9e0c4576edb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-15-3523159538.py:2: DeprecationWarning: Retrievers must implement abstract `_get_relevant_documents` method instead of `get_relevant_documents`\n","  class CustomRetriever(BaseRetriever, BaseModel):\n"]}]},{"cell_type":"markdown","source":["### Example usage of the reranking function\n"],"metadata":{"id":"Q8qGhMtsi6X_"}},{"cell_type":"code","source":["query = \"What are the impacts of climate change on biodiversity?\"\n","result = qa_chain({\"query\": query})\n","\n","print(f\"\\nQuestion: {query}\")\n","print(f\"Answer: {result['result']}\")\n","print(\"\\nRelevant source documents:\")\n","for i, doc in enumerate(result[\"source_documents\"]):\n","    print(f\"\\nDocument {i+1}:\")\n","    print(doc.page_content[:200] + \"...\")  # Print first 200 characters of each document"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rGDA23noi50B","executionInfo":{"status":"ok","timestamp":1752474868692,"user_tz":-480,"elapsed":18473,"user":{"displayName":"Ares","userId":"14709530049333005179"}},"outputId":"d04f6a7c-b59a-463a-9fd9-608c059ae9d8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Question: What are the impacts of climate change on biodiversity?\n","Answer: Climate change impacts biodiversity by altering terrestrial and marine ecosystems. In terrestrial ecosystems, it shifts habitat ranges, changes species distributions, and affects ecosystem functions, leading to a loss of biodiversity and disrupting ecological balance. In marine ecosystems, rising sea temperatures, ocean acidification, and changing currents affect marine biodiversity, disrupting food webs and fisheries. Coral reefs, in particular, are highly sensitive to these changes, experiencing bleaching and mortality, which threaten biodiversity and fisheries. Overall, these changes can lead to significant disruptions in both terrestrial and marine ecosystems.\n","\n","Relevant source documents:\n","\n","Document 1:\n","Climate change is altering terrestrial ecosystems by shifting habitat ranges, changing species \n","distributions, and impacting ecosystem functions. Forests, grasslands, and deserts are \n","experiencing shi...\n","\n","Document 2:\n","Coral reefs are highly sensitive to changes in temperature and acidity. Ocean acidification \n","and warming waters contribute to coral bleaching and mortality, threatening biodiversity and \n","fisheries. Pr...\n","\n","Document 3:\n","cultural perceptions. \n","Youth Engagement \n","Youth are vital stakeholders in climate action. Empowering young people through education, \n","activism, and leadership opportunities can drive transformative cha...\n"]}]}]}